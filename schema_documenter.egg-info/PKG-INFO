Metadata-Version: 2.4
Name: schema-documenter
Version: 0.1.0
Summary: A tool for documenting dataset schemas
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pandas>=1.3.0
Requires-Dist: pyreadstat>=1.1.0
Requires-Dist: pyarrow>=6.0.0
Requires-Dist: rpy2>=3.5.0
Requires-Dist: numpy>=1.21.0

# Schema Documenter

A Python package for documenting dataset schemas across various file formats. This tool analyzes datasets and creates detailed schema documentation files, making it easier to understand and work with different data sources.

## Features

- Supports multiple file formats:
  - CSV
  - Excel (xlsx, xls)
  - JSON
  - Stata (dta)
  - Parquet
  - RDS
- Extracts detailed schema information including:
  - Column names and data types
  - Null value counts
  - Unique value counts
  - Sample values
- Generates JSON schema documentation files
- Handles encoding issues gracefully
- Provides detailed logging

## Installation

### Using pip

```bash
pip install schema-documenter
```

### Using uv (Recommended)

[uv](https://github.com/astral-sh/uv) is a faster alternative to pip. To install with uv:
```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install schema-documenter
uv pip install schema-documenter
```

## Usage

### Command Line Usage

The package can be used directly from the command line after installation:

```bash
# Process a single file
schema-documenter data.csv

# Process a directory
schema-documenter data_directory/

# Process with verbose output
schema-documenter --verbose data_directory/

# Process with specific log level
schema-documenter --log-level DEBUG data_directory/
```

The command-line interface supports the following options:
- `--verbose` or `-v`: Enable verbose output
- `--log-level LEVEL`: Set the logging level (DEBUG, INFO, WARNING, ERROR)
- `--help` or `-h`: Show help message

Example output:
```bash
$ schema-documenter data/
INFO: Processing data/dataset1.csv
INFO: Schema saved to data/dataset1.csv.schema
INFO: Processing data/subfolder/dataset2.xlsx
INFO: Schema saved to data/subfolder/dataset2.xlsx.schema
WARNING: Skipping unsupported file: data/notes.txt
```

### Basic Usage

```python
from schema_documenter import SchemaDocumenter

# Process a single file
documenter = SchemaDocumenter("data.csv")
documenter.process_directory()

# Process a directory of files
documenter = SchemaDocumenter("data_directory/")
documenter.process_directory()
```

### Directory Processing

The script will recursively process all supported files in a directory and its subdirectories. For example:

```python
# Given this directory structure:
# data/
# ├── dataset1.csv
# ├── dataset2.xlsx
# ├── subfolder/
# │   ├── dataset3.json
# │   └── dataset4.dta
# └── empty_folder/

documenter = SchemaDocumenter("data/")
documenter.process_directory()

# This will create:
# data/
# ├── dataset1.csv.schema
# ├── dataset2.xlsx.schema
# ├── subfolder/
# │   ├── dataset3.json.schema
# │   └── dataset4.dta.schema
# └── empty_folder/
```

The script will:
- Process all supported file types (CSV, Excel, JSON, Stata, Parquet, RDS)
- Skip unsupported file types
- Skip empty files
- Create schema files in the same directory as the source files
- Preserve the directory structure
- Log warnings for any skipped files

### Example Output

For each file processed, a corresponding `.schema` file will be created. For example, `data.csv` will produce `data.csv.schema` with content like:

```json
{
    "file_name": "data.csv",
    "file_path": "/path/to/data.csv",
    "file_type": ".csv",
    "columns": [
        {
            "name": "age",
            "dtype": "int64",
            "non_null_count": 100,
            "null_count": 0,
            "unique_values": 50,
            "sample_values": [25, 30, 35]
        }
    ],
    "column_count": 1,
    "missing_columns": false
}
```

## Development

### Setting Up Development Environment

1. Clone the repository:
```bash
git clone https://github.com/yourusername/schema-documenter.git
cd schema-documenter
```

2. Create and activate a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install development dependencies:
```bash
# Using uv (recommended)
uv pip install -r requirements-dev.txt
uv pip install -e .

# Or using pip
pip install -r requirements-dev.txt
pip install -e .
```

### Running Tests

```bash
# Run all tests with coverage report
pytest

# Run specific test file
pytest tests/test_schema_documenter.py

# Run tests with verbose output
pytest -v
```

### Code Formatting and Linting

```bash
# Format code
make format

# Check code style
make lint
```

### Building the Package

The project includes a build script that handles the build process:

```bash
# Make the build script executable
chmod +x build.sh

# Run the build script
./build.sh
```

This will:
1. Clean previous builds
2. Set up a virtual environment if needed
3. Install build dependencies
4. Build the package
5. Provide instructions for publishing

### Publishing to PyPI

1. Build the package:
```bash
./build.sh
```

2. Upload to PyPI:
```bash
python -m twine upload dist/*
```

### Local Installation

After building, you can install the package locally:

```bash
# Using uv (recommended)
uv pip install dist/*.whl

# Or using pip
pip install dist/*.whl
```

## Project Structure

```
schema_documenter/
├── schema_documenter/
│   ├── __init__.py
│   └── schema_documenter.py
├── tests/
│   ├── __init__.py
│   └── test_schema_documenter.py
├── pyproject.toml
├── setup.cfg
├── requirements.txt
├── requirements-dev.txt
├── build.sh
├── LICENSE
└── README.md
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and ensure they pass
5. Submit a pull request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Troubleshooting

### Common Issues

1. **JSON Decoding Errors**: Ensure your JSON files are properly formatted and encoded.
2. **Encoding Issues with Stata Files**: The tool will automatically try different encodings.
3. **Empty Files**: The tool will log a warning and skip empty files.

### Getting Help

If you encounter any issues:
1. Check the logs (the tool uses Python's logging module)
2. Open an issue on GitHub
3. Include relevant error messages and sample data (if possible)

## Development Tools

The project uses several development tools:

- **pytest**: For testing
- **black**: For code formatting
- **isort**: For import sorting
- **flake8**: For code style checking
- **uv**: For fast package installation and management

All of these can be installed via `requirements-dev.txt`. 

